# PPO-PyTorch

Implementaci√≥n m√≠nima de PyTorch de PPO con objetivo recortado para entornos de gimnasios OpenAI.

## ‚ÑπÔ∏è Instrucciones

- Si deseas probar la red preentrenada : ejecuta `test.py` o `test_continuous.py`
- Para entrenar de nuevo : ejecuta `PPO.py` o `PPO_continuous.py`
- Todos los hiperpar√°metros estan en los archivos `PPO.py` o `PPO_continuous.py`
- Si est√° intentando entrenarlo en un entorno donde la dimensi√≥n de la acci√≥n = 1, aseg√∫rese de verificar las dimensiones del tensor en la funci√≥n de actualizaci√≥n de la clase PPO, ya que he usado `torch.squeeze ()` varias veces. `torch.squeeze ()` aprieta el tensor de manera que no haya dimensiones de longitud = 1 ([mayor informaci√≥n](https://pytorch.org/docs/stable/torch.html?highlight=torch%20squeeze#torch.squeeze)).
- N√∫mero de actores para recopilar experiencia = 1. Esto podr√≠a cambiarse creando m√∫ltiples instancias de redes ActorCritic en la clase PPO y us√°ndolas para recopilar experiencia (como A3C y PPO est√°ndar)


## üìö Referencias

- PPO [paper](https://arxiv.org/abs/1707.06347)
- [OpenAI Spinning up](https://spinningup.openai.com/en/latest/)
