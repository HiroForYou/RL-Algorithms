<h2 align="center">
<p>Aprendizaje Reforzado</p>
</h2>

<h2 align="center">
<p></p>
<img src="https://img.shields.io/badge/PyTorch%20-%23EE4C2C.svg?&style=for-the-badge&logo=PyTorch&logoColor=white" />
<img src="https://img.shields.io/badge/numpy%20-%23013243.svg?&style=for-the-badge&logo=numpy&logoColor=white" />
<img src="https://img.shields.io/badge/Made%20with-Jupyter-orange?style=for-the-badge&logo=Jupyter" />
<p></p>
</h2>




## üÜï Update
- 25/01/21: 

**Modelo DDQN agregado**: Una variante de DQN conocida como Dueling Double DQN ha sido a√±adida. La arquitectura del modelo consta de algunas capas convolucionales. Se prueba Dueling Double DQN en el entorno GYM de DOOM. 

**Documentaci√≥n**: Se ha a√±adido documentaci√≥n en espa√±ol respecto a otros algoritmos de RL: SARSA, Actor-Critic, etc.


- 23/01/21: 

**Modelos agregados**: PPO y World Models, se actualizaron las dependencias de las paqueter√≠as de tal forma que no genere conflictos a la hora de ejecutarlos desde Colab. Puedes usar los siguientes entornos con GYM: LunarLander-v2, CarRacing y BipedalWalker-v3.

## ‚ÑπÔ∏è Instrucciones
* **PPO y World Models**

Si desea ver con m√°s detalle la explicaci√≥n te√≥rica de PPO y World Models consulte `main.ipynb` o ejec√∫telo directamente desde COLAB:

<h2 align="center">
<a href="https://colab.research.google.com/github/HiroForYou/RL-Algorithms/blob/master/main.ipynb">
<img src="https://colab.research.google.com/assets/colab-badge.svg" />
</a>
</h2>

A continuaci√≥n se muestran los resultados para el algoritmo PPO:

PPO Discretizado LunarLander-v2 (1200 episodios)           |  PPO Cont√≠nuo BipedalWalker-v3 (4000 episodios)
:-------------------------:|:-------------------------:
![](src/PPO_LunarLander-v2.gif) |  ![](src/PPO_BipedalWalker-v2.gif)


Tambi√©n mostramos como se ver√≠a la salida del enfoque World Models en el circuito de coches:
<p align="center">
  <img src="src/car.gif" />
  <p align="center">"Reconstuyendo los sue√±os"</p>
</p>

* **DQN**

Para esta parte me base en el repositorio de [EXJUSTICE](https://github.com/EXJUSTICE). De momento la versi√≥n disponible en espa√±ol es la variante Dueling Double DQN o DDDQN, que logra un rendimiento superior que el enfoque inicial DQN.

Si desea consultar el cuaderno de DDQN, consulte `DQN/DDDQN/Doom-DDDQN-Pytorch.ipynb` o ejec√∫telo directamente desde COLAB:

<h2 align="center">
<a href="https://colab.research.google.com/github/HiroForYou/RL-Algorithms/blob/master/DQN/DDDQN/Doom-DDDQN-Pytorch.ipynb">
<img src="https://colab.research.google.com/assets/colab-badge.svg" />
</a>
</h2>

Tambi√©n mostramos como se ver√≠a la salida del enfoque DDDQN dentro del juego DOOM para 2000 episodios:
<p align="center">
  <img src="src/DDDQN_doom_2000_episodios.gif" />
  <p align="center">DDQN en DOOM</p>
</p>

*Nota*: Si usted ejecuta el cuaderno desde COLAB observar√° un error en la celda de entrenamiento. Hasta el momento de la actualizaci√≥n de este repositorio, no se ha podido fijar el error. He abierto un hilo [issues](https://github.com/EXJUSTICE/Doom_DQN_GC/issues/1) en el repositorio original del que me bas√© para poder arreglar esto. 

* **Otros**

Para mayor informaci√≥n en espa√±ol de algunos algoritmos de RL consulte la carpeta `Documents-Theory`. Me bas√© en el trabajo de [Lilian Weng](https://lilianweng.github.io/lil-log/).
<p align="center">
  <img src="src/documentation.png" />
  <p align="center">Mayor informaci√≥n de algunos algoritmos RL</p>
</p>

## üë®‚Äçüíª Maintainers
* Cristhian Wiki, Github: [HiroForYou](https://github.com/HiroForYou) Email: csanchezs@uni.pe

## üôèüèΩ Agradecimientos
* Version 0.1:
Este repositorio se baso principalmente en estos art√≠culos de [towardsdatascience](https://towardsdatascience.com/introduction-to-proximal-policy-optimization-tutorial-with-openai-gym-environment-d1d80036e7c2) y [medium](https://medium.com/arxiv-bytes/summary-world-models-b050be1bf2d5) para el enfoque te√≥rico de PPO y World Models, le recomiendo encarecidamente invierta su tiempo en dichas lecturas, son una buena apuesta!
En cuanto al c√≥digo para PPO y World Models, me base en los siguientes repositorios de [pytorch-vae](https://github.com/sksq96/pytorch-vae.git) y [PPO-PyTorch](https://github.com/nikhilbarhate99/PPO-PyTorch.git). Yo solo hice algunas modificaciones a por cuestiones de incompatibilidades de Pyglet, Gym y Box2D, de tal forma que puedan ejecutarse sin problemas desde Google Colab. 

* Version 0.2:
*Soon*